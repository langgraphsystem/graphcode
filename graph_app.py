# graph_app.py
from __future__ import annotations
import os, re, time, hashlib, sqlite3, zipfile, json, logging
from pathlib import Path
from typing import TypedDict, Optional, Iterable

from langgraph.graph import StateGraph, END

# –ß–µ–∫–ø–æ–π–Ω—Ç–µ—Ä: —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º SQLite, –µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º MemorySaver
try:
    from langgraph.checkpoint.sqlite import SqliteSaver
    _CHECKPOINTER_KIND = "sqlite"
except Exception:
    from langgraph.checkpoint.memory import MemorySaver
    _CHECKPOINTER_KIND = "memory"

from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from pydantic import BaseModel
from openai import OpenAI

# ---------- –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø ----------
logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO"),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ---------- –ü–£–¢–ò/–ù–ê–°–¢–†–û–ô–ö–ò ----------
OUTPUT_DIR = Path(os.getenv("OUTPUT_DIR", "./out")).resolve()
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ GPT-5 –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å
DEFAULT_MODEL = "gpt-5"
VALID_MODELS = {"gpt-5"}  # –¢–æ–ª—å–∫–æ GPT-5 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è

REQUEST_TIMEOUT = int(os.getenv("OPENAI_REQUEST_TIMEOUT", "300"))

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã PROMPT-ADAPTER (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –≤ Railway Variables)
ADAPTER_MODEL = os.getenv("ADAPTER_MODEL", DEFAULT_MODEL)
CODEGEN_MODEL = os.getenv("CODEGEN_MODEL", DEFAULT_MODEL)
ADAPTER_TARGETS = os.getenv("ADAPTER_TARGETS", "Python 3.11; Ruff+Black; Pydantic v2; asyncio; type hints strict")
ADAPTER_CONSTRAINTS = os.getenv("ADAPTER_CONSTRAINTS", "No secrets; reasonable perf; minimal deps")
ADAPTER_TEST_POLICY = os.getenv("ADAPTER_TEST_POLICY", "NO_TESTS")
ADAPTER_OUTPUT_LANG = os.getenv("ADAPTER_OUTPUT_LANG", "EN")
ADAPTER_OUTPUT_PREF = os.getenv("ADAPTER_OUTPUT_PREF", "FILES_JSON")  # FILES_JSON | UNIFIED_DIFF | TOOLS_CALLS

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç OpenAI
openai_client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    timeout=REQUEST_TIMEOUT
)

# ---------- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –£–¢–ò–õ–ò–¢–´ ----------
EXT2LANG = {
    ".py":"python", ".js":"javascript", ".ts":"typescript", ".html":"html",
    ".css":"css", ".json":"json", ".yml":"yaml", ".yaml":"yaml",
    ".sh":"bash", ".sql":"sql", ".txt":"text", ".rs":"rust",
    ".go":"go", ".java":"java", ".cpp":"cpp", ".c":"c",
}

def detect_language(filename: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é —Ñ–∞–π–ª–∞"""
    return EXT2LANG.get(Path(filename).suffix.lower(), "text")

def sanitize_filename(filename: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –æ—Ç –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    unsafe_chars = ['..', '/', '\\', ':', '*', '?', '"', '<', '>', '|', '\x00']
    clean_name = filename
    for char in unsafe_chars:
        clean_name = clean_name.replace(char, '_')
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
    max_length = 255
    if len(clean_name) > max_length:
        name, ext = os.path.splitext(clean_name)
        clean_name = name[:max_length - len(ext)] + ext
    
    # –ï—Å–ª–∏ –∏–º—è –ø—É—Å—Ç–æ–µ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
    if not clean_name or clean_name.strip() in ['.', '..']:
        clean_name = 'unnamed_file'
    
    return clean_name.strip()

def safe_path_join(base_dir: Path, relative_path: str) -> Optional[Path]:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—É—Ç–µ–π —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤—ã—Ö–æ–¥–∞ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –±–∞–∑–æ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    try:
        clean_path = relative_path.strip().lstrip('/\\')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ path traversal
        if '..' in clean_path or clean_path.startswith('/'):
            logger.warning(f"Potentially unsafe path rejected: {relative_path}")
            return None
        
        full_path = base_dir / clean_path
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—É—Ç—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –±–∞–∑–æ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        try:
            full_path.resolve().relative_to(base_dir.resolve())
        except ValueError:
            logger.warning(f"Path outside base directory rejected: {relative_path}")
            return None
        
        return full_path
        
    except Exception as e:
        logger.error(f"Error processing path: {e}")
        return None

def chat_dir(chat_id: int) -> Path:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞"""
    p = OUTPUT_DIR / str(chat_id)
    p.mkdir(parents=True, exist_ok=True)
    return p

def latest_path(chat_id: int, filename: str) -> Path:
    """–ü—É—Ç—å –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏ —Ñ–∞–π–ª–∞"""
    return chat_dir(chat_id) / f"latest-{filename}"

def ensure_latest_placeholder(chat_id: int, filename: str, language: str) -> Path:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞-–∑–∞–≥–ª—É—à–∫–∏ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç"""
    lp = latest_path(chat_id, filename)
    if lp.exists():
        return lp
    
    stubs = {
        'python':      "# -*- coding: utf-8 -*-\n# created via /create\n",
        'javascript':  "// created via /create\n",
        'typescript':  "// created via /create\n",
        'html':        "<!-- created via /create -->\n",
        'css':         "/* created via /create */\n",
        'json':        "{\n}\n",
        'yaml':        "# created via /create\n",
        'bash':        "#!/usr/bin/env bash\n# created via /create\n",
        'sql':         "-- created via /create\n",
        'rust':        "// created via /create\n",
        'go':          "// created via /create\npackage main\n",
        'java':        "// created via /create\npublic class Main {}\n",
        'text':        "",
    }
    
    try:
        lp.write_text(stubs.get(language, ""), encoding="utf-8")
    except Exception:
        lp.touch()
    return lp

def list_files(chat_id: int) -> list[str]:
    """–°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞"""
    base = chat_dir(chat_id)
    return sorted([p.name for p in base.iterdir() if p.is_file()])

def _sha256_bytes(data: bytes) -> str:
    """SHA256 —Ö–µ—à –±–∞–π—Ç–æ–≤"""
    return hashlib.sha256(data).hexdigest()

def _sha256_file(path: Path) -> str:
    """SHA256 —Ö–µ—à —Ñ–∞–π–ª–∞"""
    return _sha256_bytes(path.read_bytes())

def version_current_file(chat_id: int, filename: str, new_content: str) -> Path:
    """–í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π"""
    lp = latest_path(chat_id, filename)
    old = lp.read_text(encoding="utf-8") if lp.exists() else ""
    
    # –ï—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å, –Ω–µ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é
    if hashlib.sha256(old.encode()).hexdigest() == hashlib.sha256(new_content.encode()).hexdigest():
        return lp
    
    # –°–æ–∑–¥–∞–µ–º –≤–µ—Ä—Å–∏—é —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
    ts = time.strftime("%Y%m%d-%H%M%S")
    ver = chat_dir(chat_id) / f"{ts}-{filename}"
    ver.write_text(new_content, encoding="utf-8")
    lp.write_text(new_content, encoding="utf-8")
    
    logger.info(f"Created version: {ver.name}")
    return lp

# --- –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–¥–∞/–¥–∏—Ñ—Ñ–∞ ---
CODE_BLOCK_RE = re.compile(r"```([a-zA-Z0-9_+-]+)?\n(.*?)```", re.DOTALL)
DIFF_BLOCK_RE = re.compile(r"```(diff|patch)\n(.*?)```", re.DOTALL | re.IGNORECASE)
UNIFIED_DIFF_HINT_RE = re.compile(r"(?m)^(--- |\+\+\+ |@@ )")
GIT_DIFF_HINT_RE = re.compile(r"(?m)^diff --git ")

def extract_code(text: str) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–¥–∞ –∏–∑ markdown –±–ª–æ–∫–∞"""
    m = CODE_BLOCK_RE.search(text)
    if not m:
        return text.strip()
    return m.group(2).strip()

def extract_diff_and_spec(text: str) -> tuple[str, str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ diff –∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    diff_parts: list[str] = []
    def _grab(m: re.Match) -> str:
        diff_parts.append(m.group(2).strip())
        return ""
    text_wo = DIFF_BLOCK_RE.sub(_grab, text)
    diff_text = "\n\n".join(diff_parts).strip()
    
    if not diff_text and (GIT_DIFF_HINT_RE.search(text_wo) or UNIFIED_DIFF_HINT_RE.search(text_wo)):
        return "", text_wo.strip()
    
    return text_wo.strip(), diff_text

PLACEHOLDER_HINT = "created via /create"

def _is_placeholder_or_empty(content: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –∑–∞–≥–ª—É—à–∫–æ–π"""
    if not content.strip(): 
        return True
    if PLACEHOLDER_HINT in content: 
        return True
    return len(content.strip()) < 8

# ---------- OpenAI API –í–´–ó–û–í–´ ----------
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=8),
    retry=retry_if_exception_type(Exception),
    reraise=True
)
def _openai_create(model: str, input_payload):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–∑–æ–≤–∞ OpenAI API —Å GPT-5
    """
    # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º GPT-5
    model = "gpt-5"
    
    # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—å —Å messages
    if isinstance(input_payload, list):
        messages = input_payload
    elif isinstance(input_payload, dict) and "messages" in input_payload:
        messages = input_payload["messages"]
    else:
        # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞, –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç chat completion
        messages = [{"role": "user", "content": str(input_payload)}]
    
    try:
        logger.info(f"Calling OpenAI API with model {model}")
        response = openai_client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=4096,
            temperature=0.2,
        )
        
        return response
        
    except Exception as e:
        logger.error(f"OpenAI API error: {e}")
        raise

# ---------- PROMPT-ADAPTER V3 –®–ê–ë–õ–û–ù ----------
PROMPT_ADAPTER_V3 = r"""[PROMPT-ADAPTER v3 ‚Äî EN-adapt, API-ready for GPT-5]
[STATIC RULES ‚Äî cacheable]
You are a PromptAdapter for code generation via OpenAI API (GPT-5). Your job: take RAW_TASK (any language) + CONTEXT and return an API-ready package with:
- clean English developer instructions,
- user message containing both original content and an English adaptation,
- a strict response contract (FILES_JSON | UNIFIED_DIFF | TOOLS_CALLS).

Principles:
1) Role separation: put rules in developer; data/context in user. 
2) Output must follow the selected mode exactly (no extra prose).
3) Minimal necessary context: do not invent files not provided.
4) Short plan (3‚Äì6 steps), no chain-of-thought.
5) If inputs are incomplete, state careful assumptions explicitly.
6) For multi-file changes use TOOLS_CALLS with atomic tool calls.
7) Limit "verbal text" to ‚â§200 lines outside code/DIFF.

**English Adaptation Policy:**
- Translate instructions/specs/requirements to English concisely.
- DO NOT translate: code blocks, stack traces, file paths, API names, JSON/YAML, diffs, UI strings, domain terms.
- Keep user-facing strings in OUTPUT_LANG if specified.

[OUTPUT SCHEMA ‚Äî return ONE JSON object]
{
  "messages": [
    {"role": "developer", "content": "string"},
    {"role": "user", "content": "string"}
  ],
  "response_contract": {
    "mode": "FILES_JSON | UNIFIED_DIFF | TOOLS_CALLS",
    "structured_outputs_schema": {
      "type":"object",
      "properties":{
        "files":{
          "type":"array",
          "items":{
            "type":"object",
            "properties":{
              "path":{"type":"string"},
              "content":{"type":"string"}
            },
            "required":["path","content"],
            "additionalProperties":false
          }
        },
        "notes":{"type":"string"}
      },
      "required":["files"],
      "additionalProperties":false
    }
  },
  "runbook":{
    "plan":["step 1","step 2","step 3"],
    "commands":["<install/build/test cmds>"],
    "tests_hint":"what to cover if TEST_POLICY=TDD"
  },
  "assumptions":["list of assumptions"],
  "risks":["edge cases & risks"],
  "notes":"short CI/CD hints"
}

[DYNAMIC INPUT]
RAW_TASK: <<<RAW_TASK>>>
{RAW_TASK}
<<<END>>>
CONTEXT: <<<CONTEXT>>>
{CONTEXT}
<<<END>>>
MODE: {MODE}
TARGETS: {TARGETS}
CONSTRAINTS: {CONSTRAINTS}
TEST_POLICY: {TEST_POLICY}
OUTPUT_PREF: {OUTPUT_PREF}
OUTPUT_LANG: {OUTPUT_LANG}

[NOW DO]
Construct and return ONE JSON object strictly matching OUTPUT SCHEMA.
"""

def _build_context_block(chat_id: int, filename: str) -> str:
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –±–ª–æ–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ñ–∞–π–ª–∞"""
    lp = latest_path(chat_id, filename)
    if not lp.exists():
        return ""
    lang = detect_language(filename)
    code = lp.read_text(encoding="utf-8")
    return f"<<<CONTEXT:FILE {filename}>>>\n```{lang}\n{code}\n```\n<<<END>>>"

def _call_adapter(raw_task: str, context_block: str, mode_tag: str, output_pref: str) -> dict:
    """–í—ã–∑–æ–≤ PROMPT-ADAPTER –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –ø—Ä–æ–º–ø—Ç–∞"""
    adapter_prompt = PROMPT_ADAPTER_V3.format(
        RAW_TASK=raw_task,
        CONTEXT=context_block or "(none)",
        MODE=mode_tag,
        TARGETS=ADAPTER_TARGETS,
        CONSTRAINTS=ADAPTER_CONSTRAINTS,
        TEST_POLICY=ADAPTER_TEST_POLICY,
        OUTPUT_PREF=output_pref,
        OUTPUT_LANG=ADAPTER_OUTPUT_LANG,
    )
    
    try:
        # –í—ã–∑—ã–≤–∞–µ–º OpenAI API —Å GPT-5
        response = _openai_create(ADAPTER_MODEL, adapter_prompt)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
        if hasattr(response, 'choices') and response.choices:
            text = response.choices[0].message.content
        else:
            raise ValueError("Invalid response format from OpenAI")
        
        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            # –ï—Å–ª–∏ JSON –æ–±–µ—Ä–Ω—É—Ç –≤ markdown –±–ª–æ–∫ –∫–æ–¥–∞
            code_block_match = re.search(r'```(?:json)?\n(.*?)\n```', text, re.DOTALL)
            if code_block_match:
                return json.loads(code_block_match.group(1))
            
            # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –∏–∑–≤–ª–µ—á—å —á–µ—Ä–µ–∑ extract_code
            inner = extract_code(text)
            return json.loads(inner)
            
    except Exception as e:
        logger.error(f"Adapter call failed: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–∞–ª–∏–¥–Ω—ã–π –æ–±—ä–µ–∫—Ç
        return {
            "messages": [
                {"role": "system", "content": "Generate code based on user request using GPT-5"},
                {"role": "user", "content": raw_task}
            ],
            "response_contract": {"mode": output_pref}
        }

def _call_codegen_from_messages(messages: list[dict]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    try:
        response = _openai_create(CODEGEN_MODEL, messages)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
        if hasattr(response, 'choices') and response.choices:
            text = response.choices[0].message.content
            return text
        else:
            raise ValueError("Invalid response format from OpenAI")
            
    except Exception as e:
        logger.error(f"Codegen call failed: {e}")
        return "# Error generating code"

def _apply_files_json(chat_id: int, active_filename: str, files_obj: list[dict]) -> Path:
    """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ FILES_JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –ø—É—Ç–µ–π"""
    active_written = None
    base_dir = chat_dir(chat_id)
    
    for item in files_obj:
        raw_path = item.get("path", "").strip()
        content = item.get("content", "")
        
        if not raw_path:
            continue
        
        # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É—Ç–∏
        safe_output_path = safe_path_join(base_dir, raw_path)
        if safe_output_path is None:
            logger.warning(f"Skipping unsafe path: {raw_path}")
            continue
        
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            safe_output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            safe_output_path.write_text(content, encoding="utf-8")
            logger.info(f"Written file: {safe_output_path}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –∞–∫—Ç–∏–≤–Ω—ã–º —Ñ–∞–π–ª–æ–º
            if Path(raw_path).name == active_filename:
                active_written = version_current_file(chat_id, active_filename, content)
                
        except Exception as e:
            logger.error(f"Failed to write file {raw_path}: {e}")
            continue
    
    # –ï—Å–ª–∏ –∞–∫—Ç–∏–≤–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –±—ã–ª –∑–∞–ø–∏—Å–∞–Ω, –Ω–æ –µ—Å—Ç—å —Ñ–∞–π–ª—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π
    if active_written is None and files_obj:
        first = files_obj[0]
        content = first.get("content", "")
        active_written = version_current_file(chat_id, active_filename, content)
    
    return active_written or latest_path(chat_id, active_filename)

def _infer_output_pref(raw_text: str, has_context: bool) -> str:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –≤—ã–≤–æ–¥–∞"""
    if has_context and (DIFF_BLOCK_RE.search(raw_text) or UNIFIED_DIFF_HINT_RE.search(raw_text) or GIT_DIFF_HINT_RE.search(raw_text)):
        return "UNIFIED_DIFF"
    return ADAPTER_OUTPUT_PREF

# ---------- AUDIT LOG ----------
AUDIT_DB = OUTPUT_DIR / "audit.db"

def _audit_connect():
    """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î –∞—É–¥–∏—Ç–∞"""
    conn = sqlite3.connect(AUDIT_DB)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS events (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            ts TEXT NOT NULL,
            chat_id INTEGER NOT NULL,
            event_type TEXT NOT NULL,
            active_file TEXT,
            model TEXT,
            prompt TEXT,
            output_path TEXT,
            output_sha256 TEXT,
            output_bytes INTEGER,
            meta TEXT
        )
    """)
    return conn

def _truncate(s: Optional[str], limit: int = 4000) -> Optional[str]:
    """–û–±—Ä–µ–∑–∫–∞ —Å—Ç—Ä–æ–∫–∏ –¥–æ –ª–∏–º–∏—Ç–∞"""
    if s is None: return None
    if len(s) <= limit: return s
    return s[:limit]

def _file_meta(path: Optional[Path]) -> tuple[Optional[str], Optional[int]]:
    """–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞ (—Ö–µ—à –∏ —Ä–∞–∑–º–µ—Ä)"""
    if not path or not path.exists():
        return None, None
    b = path.stat().st_size
    h = _sha256_file(path)
    return h, b

def audit_event(chat_id: int, event_type: str, active_file: Optional[str] = None,
                model: Optional[str] = None, prompt: Optional[str] = None,
                output_path: Optional[Path] = None, meta: Optional[dict] = None):
    """–ó–∞–ø–∏—Å—å —Å–æ–±—ã—Ç–∏—è –≤ –∞—É–¥–∏—Ç –ª–æ–≥"""
    conn = _audit_connect()
    try:
        ts = time.strftime("%Y-%m-%d %H:%M:%S")
        sha, size = _file_meta(output_path)
        conn.execute(
            "INSERT INTO events (ts, chat_id, event_type, active_file, model, prompt, output_path, output_sha256, output_bytes, meta)"
            " VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
            (ts, chat_id, event_type, active_file, model, _truncate(prompt), str(output_path) if output_path else None, sha, size, json.dumps(meta or {}))
        )
        conn.commit()
    finally:
        conn.close()

# ---------- –°–û–°–¢–û–Ø–ù–ò–ï ----------
class InMsg(BaseModel):
    chat_id: int
    text: str

class GraphState(TypedDict, total=False):
    chat_id: int
    input_text: str
    command: str               # CREATE | SWITCH | FILES | MODEL | RESET | GENERATE | DOWNLOAD
    arg: Optional[str]
    active_file: Optional[str]
    model: str
    reply_text: str
    file_to_send: Optional[str]

# ---------- –î–ï–ö–û–†–ê–¢–û–† –î–õ–Ø –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–ò –£–ó–õ–û–í ----------
def safe_node(func):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ –≤ —É–∑–ª–∞—Ö –≥—Ä–∞—Ñ–∞"""
    def wrapper(state: GraphState) -> GraphState:
        try:
            return func(state)
        except Exception as e:
            logger.error(f"Error in {func.__name__}: {e}", exc_info=True)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–Ω—è—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –≤ {func.__name__}: "
            
            if "api_key" in str(e).lower():
                error_msg += "–ü—Ä–æ–±–ª–µ–º–∞ —Å API –∫–ª—é—á–æ–º OpenAI"
            elif "rate" in str(e).lower():
                error_msg += "–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API"
            elif "timeout" in str(e).lower():
                error_msg += "–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞"
            elif "json" in str(e).lower():
                error_msg += "–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç AI"
            else:
                error_msg += str(e)[:200]
            
            state["reply_text"] = error_msg
            return state
    
    wrapper.__name__ = func.__name__
    return wrapper

# ---------- –£–ó–õ–´ –ì–†–ê–§–ê ----------
def parse_message(state: GraphState) -> GraphState:
    """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Ö–æ–¥—è—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã"""
    text = state["input_text"].strip()
    state["command"] = "GENERATE"
    state["arg"] = None
    
    if text.startswith("/"):
        parts = text.split(maxsplit=1)
        cmd = parts[0].lower()
        arg = parts[1].strip() if len(parts) > 1 else None
        
        mapping = {
            "/create": "CREATE",
            "/switch": "SWITCH", 
            "/files": "FILES",
            "/model": "MODEL",
            "/reset": "RESET",
            "/download": "DOWNLOAD"
        }
        state["command"] = mapping.get(cmd, "GENERATE")
        state["arg"] = arg
    
    return state

@safe_node
def node_create(state: GraphState) -> GraphState:
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –∏–º–µ–Ω–µ–º"""
    chat_id = state["chat_id"]
    raw_filename = (state.get("arg") or "main.py").strip()
    
    # –û—á–∏—â–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞
    filename = sanitize_filename(raw_filename)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫
    language = detect_language(filename)
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª-–∑–∞–≥–ª—É—à–∫—É
    ensure_latest_placeholder(chat_id, filename, language)
    
    state["active_file"] = filename
    state["reply_text"] = f"‚úÖ –§–∞–π–ª —Å–æ–∑–¥–∞–Ω/–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω: `{filename}`\nüî§ –Ø–∑—ã–∫: {language}"
    
    # –ï—Å–ª–∏ –∏–º—è –±—ã–ª–æ –∏–∑–º–µ–Ω–µ–Ω–æ, —Å–æ–æ–±—â–∞–µ–º
    if filename != raw_filename:
        state["reply_text"] += f"\n‚ö†Ô∏è –ò–º—è —Ñ–∞–π–ª–∞ –±—ã–ª–æ –æ—á–∏—â–µ–Ω–æ –æ—Ç –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"
    
    audit_event(chat_id, "CREATE", active_file=filename, model=state.get("model"))
    return state

@safe_node
def node_switch(state: GraphState) -> GraphState:
    """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª"""
    chat_id = state["chat_id"]
    filename = (state.get("arg") or "").strip()
    
    if not filename:
        state["reply_text"] = "–£–∫–∞–∂–∏ –∏–º—è: `/switch app.py`"
        return state
    
    # –û—á–∏—â–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞
    filename = sanitize_filename(filename)
    
    if not latest_path(chat_id, filename).exists():
        state["reply_text"] = f"–§–∞–π–ª `{filename}` –µ—â—ë –Ω–µ —Å–æ–∑–¥–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–π `/create {filename}`."
        return state
    
    state["active_file"] = filename
    state["reply_text"] = f"üîÄ –ü–µ—Ä–µ–∫–ª—é—á–∏–ª—Å—è –Ω–∞ `{filename}`."
    audit_event(chat_id, "SWITCH", active_file=filename, model=state.get("model"))
    return state

@safe_node
def node_files(state: GraphState) -> GraphState:
    """–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤"""
    files = list_files(state["chat_id"])
    if not files:
        state["reply_text"] = "–§–∞–π–ª–æ–≤ –ø–æ–∫–∞ –Ω–µ—Ç. –ù–∞—á–Ω–∏ —Å `/create app.py`."
    else:
        state["reply_text"] = "üóÇ –§–∞–π–ª—ã:\n" + "\n".join(f"- {f}" for f in files)
    audit_event(state["chat_id"], "FILES", active_file=state.get("active_file"), model=state.get("model"))
    return state

@safe_node
def node_model(state: GraphState) -> GraphState:
    """–ü–æ–∫–∞–∑–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ (—Ç–æ–ª—å–∫–æ GPT-5)"""
    state["reply_text"] = (
        f"üß† –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å: `GPT-5`\n\n"
        f"‚ÑπÔ∏è –≠—Ç–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –º–æ–¥–µ–ª—å.\n"
        f"–í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –Ω–∞ GPT-5."
    )
    audit_event(state["chat_id"], "MODEL", active_file=state.get("active_file"), model="gpt-5")
    return state

@safe_node
def node_reset(state: GraphState) -> GraphState:
    """–°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è —á–∞—Ç–∞"""
    state["active_file"] = None
    state["model"] = DEFAULT_MODEL
    state["reply_text"] = "‚ôªÔ∏è –°–±—Ä–æ—Å–∏–ª —Å–æ—Å—Ç–æ—è–Ω–∏–µ —á–∞—Ç–∞. –ù–∞—á–Ω–∏ —Å `/create <filename>`."
    audit_event(state["chat_id"], "RESET")
    return state

@safe_node
def node_generate(state: GraphState) -> GraphState:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é GPT-5"""
    chat_id = state["chat_id"]
    active = state.get("active_file")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if not active:
        active = "main.py"
        ensure_latest_placeholder(chat_id, active, detect_language(active))
        state["active_file"] = active
        logger.info(f"Auto-created file: {active}")

    # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º GPT-5
    model = "gpt-5"
    state["model"] = model
    
    raw_user_text = state["input_text"]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞
    lp = latest_path(chat_id, active)
    existed_before = lp.exists()
    ensure_latest_placeholder(chat_id, active, detect_language(active))
    current_text = lp.read_text(encoding="utf-8") if lp.exists() else ""

    language = detect_language(active)
    has_context = existed_before and not _is_placeholder_or_empty(current_text)
    context_block = _build_context_block(chat_id, active) if has_context else ""
    mode_tag = "DIFF_PATCH" if has_context else "NEW_FILE"
    output_pref = _infer_output_pref(raw_user_text, has_context)

    logger.info(f"Generating for {active} with GPT-5, mode {mode_tag}")

    try:
        # –®–∞–≥ 1: PROMPT-ADAPTER
        adapter_obj = _call_adapter(raw_user_text, context_block, mode_tag, output_pref)
        messages = adapter_obj.get("messages") or []
        
        if not messages:
            raise ValueError("Adapter returned empty messages")

        # –®–∞–≥ 2: Codegen —Å GPT-5
        codegen_text = _call_codegen_from_messages(messages)
        
        if not codegen_text or codegen_text == "# Error generating code":
            raise ValueError("Failed to generate code")

        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        mode = (adapter_obj.get("response_contract") or {}).get("mode", output_pref)
        updated_path = None

        if (mode or "").upper() == "FILES_JSON":
            try:
                obj = json.loads(codegen_text)
            except json.JSONDecodeError:
                obj = json.loads(extract_code(codegen_text))
            
            files = obj.get("files") or []
            if not files:
                raise ValueError("No files in response")
                
            updated_path = _apply_files_json(chat_id, active, files)

        elif (mode or "").upper() == "UNIFIED_DIFF":
            # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –Ω–∞ –ø–æ–ª–Ω—ã–π —Ñ–∞–π–ª
            logger.info("UNIFIED_DIFF mode - using full file replacement")
            code = extract_code(codegen_text)
            updated_path = version_current_file(chat_id, active, code)

        else:
            # –†–µ–∂–∏–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é - –ø—Ä–æ—Å—Ç–æ–π –∫–æ–¥
            code = extract_code(codegen_text)
            updated_path = version_current_file(chat_id, active, code)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        rel = latest_path(chat_id, active).relative_to(OUTPUT_DIR)
        state["reply_text"] = (
            f"‚úÖ –û–±–Ω–æ–≤–∏–ª `{active}` —á–µ—Ä–µ–∑ PROMPT-ADAPTER v3\n"
            f"üß† –ú–æ–¥–µ–ª—å: `GPT-5`\n"
            f"üìÅ –ö–æ–Ω—Ç—Ä–∞–∫—Ç: `{(mode or output_pref)}`\n"
            f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: `{rel}`\n\n"
            f"–û—Ç–ø—Ä–∞–≤—å —Å–ª–µ–¥—É—é—â–∏–π –ø—Ä–æ–º–ø—Ç –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π –∫–æ–º–∞–Ω–¥—ã."
        )
        
        # –ê—É–¥–∏—Ç —É—Å–ø–µ—à–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        audit_event(
            chat_id, "GENERATE",
            active_file=active, 
            model="gpt-5",
            prompt=json.dumps(adapter_obj, ensure_ascii=False)[:4000],
            output_path=updated_path,
            meta={
                "adapter_mode": mode_tag, 
                "contract_mode": mode or output_pref,
                "success": True
            }
        )
        
    except Exception as e:
        logger.error(f"Generation failed: {e}", exc_info=True)
        
        # –ê—É–¥–∏—Ç –Ω–µ—É–¥–∞—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        audit_event(
            chat_id, "GENERATE_ERROR",
            active_file=active,
            model="gpt-5",
            meta={"error": str(e)[:500]}
        )
        
        # –ü–µ—Ä–µ–±—Ä–∞—Å—ã–≤–∞–µ–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–æ–º
        raise

    return state

# --------- DOWNLOAD ---------
def _iter_selected_files(base: Path, arg: Optional[str]) -> Iterable[Path]:
    """–ò—Ç–µ—Ä–∞—Ç–æ—Ä –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º"""
    files = [p for p in base.iterdir() if p.is_file()]
    if not arg:
        return sorted(files)
    
    arg = arg.strip().lower()
    if arg == "latest":
        return sorted([p for p in files if p.name.startswith("latest-")])
    if arg == "versions":
        return sorted([p for p in files if not p.name.startswith("latest-")])
    
    return sorted([p for p in files if p.name == f"latest-{arg}" or p.name.endswith(f"-{arg}")])

def _make_zip(chat_id: int, arg: Optional[str]) -> Path:
    """–°–æ–∑–¥–∞–Ω–∏–µ ZIP –∞—Ä—Ö–∏–≤–∞ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏"""
    base = chat_dir(chat_id)
    ts = time.strftime("%Y%m%d-%H%M%S")
    out = base / f"export-{ts}.zip"
    to_pack = list(_iter_selected_files(base, arg))
    
    with zipfile.ZipFile(out, "w", compression=zipfile.ZIP_DEFLATED) as z:
        for p in to_pack:
            z.write(p, arcname=p.name)
    
    logger.info(f"Created ZIP archive: {out.name} with {len(to_pack)} files")
    return out

@safe_node
def node_download(state: GraphState) -> GraphState:
    """–°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è"""
    chat_id = state["chat_id"]
    arg = state.get("arg")
    
    try:
        z = _make_zip(chat_id, arg)
        state["file_to_send"] = str(z)
        sel = arg or "all"
        state["reply_text"] = f"üì¶ –ü–æ–¥–≥–æ—Ç–æ–≤–∏–ª –∞—Ä—Ö–∏–≤ `{z.name}` ({sel})."
        audit_event(chat_id, "DOWNLOAD", active_file=state.get("active_file"), model="gpt-5", output_path=z)
    except Exception as e:
        logger.error(f"Failed to create archive: {e}")
        state["reply_text"] = f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∞—Ä—Ö–∏–≤: {str(e)[:200]}"
    
    return state

# ---------- –†–û–£–¢–ï–† –ò –°–ë–û–†–ö–ê –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø ----------
def router(state: GraphState) -> str:
    """–†–æ—É—Ç–µ—Ä –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —É–∑–ª–∞"""
    return state["command"]

def build_app():
    """–°–±–æ—Ä–∫–∞ LangGraph –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å checkpointer"""
    sg = StateGraph(GraphState)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–ª—ã
    sg.add_node("parse", parse_message)
    sg.add_node("CREATE", node_create)
    sg.add_node("SWITCH", node_switch)
    sg.add_node("FILES", node_files)
    sg.add_node("MODEL", node_model)
    sg.add_node("RESET", node_reset)
    sg.add_node("GENERATE", node_generate)
    sg.add_node("DOWNLOAD", node_download)

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ—á–∫—É –≤—Ö–æ–¥–∞
    sg.set_entry_point("parse")
    
    # –î–æ–±–∞–≤–ª—è–µ–º —É—Å–ª–æ–≤–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã –æ—Ç –ø–∞—Ä—Å–µ—Ä–∞ –∫ —É–∑–ª–∞–º
    sg.add_conditional_edges("parse", router, {
        "CREATE": "CREATE",
        "SWITCH": "SWITCH",
        "FILES": "FILES",
        "MODEL": "MODEL",
        "RESET": "RESET",
        "GENERATE": "GENERATE",
        "DOWNLOAD": "DOWNLOAD",
    })
    
    # –í—Å–µ —É–∑–ª—ã –≤–µ–¥—É—Ç –∫ –∫–æ–Ω—Ü—É
    for node in ("CREATE", "SWITCH", "FILES", "MODEL", "RESET", "GENERATE", "DOWNLOAD"):
        sg.add_edge(node, END)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ checkpointer
    if _CHECKPOINTER_KIND == "sqlite":
        try:
            from langgraph.checkpoint.sqlite import SqliteSaver
            
            db_path = OUTPUT_DIR / "langgraph.db"
            db_path.parent.mkdir(parents=True, exist_ok=True)
            
            # –°–æ–∑–¥–∞–µ–º checkpointer —Å SQLite
            checkpointer = SqliteSaver(str(db_path))
            
            logger.info(f"Using SQLite checkpointer at {db_path}")
            
        except Exception as e:
            logger.warning(f"Failed to create SQLite checkpointer: {e}, falling back to MemorySaver")
            from langgraph.checkpoint.memory import MemorySaver
            checkpointer = MemorySaver()
    else:
        from langgraph.checkpoint.memory import MemorySaver
        checkpointer = MemorySaver()
        logger.info("Using MemorySaver (non-persistent)")

    # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –≥—Ä–∞—Ñ —Å checkpointer
    compiled_app = sg.compile(checkpointer=checkpointer)
    
    logger.info("LangGraph application compiled successfully with GPT-5 support")
    return compiled_app

# ---------- –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ----------
APP = build_app()

# –≠–∫—Å–ø–æ—Ä—Ç –¥–ª—è bot.py
__all__ = ['APP', 'DEFAULT_MODEL', 'VALID_MODELS']

logger.info(f"Graph app initialized. Model: GPT-5 only. Output dir: {OUTPUT_DIR}")
