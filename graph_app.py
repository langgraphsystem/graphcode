from **future** import annotations
import os, re, time, hashlib, sqlite3, zipfile, json, logging
from pathlib import Path
from typing import TypedDict, Optional, Iterable, List, Dict, Any, Tuple
from dataclasses import dataclass
from enum import Enum

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from pydantic import BaseModel, Field, validator
from openai import OpenAI

# Optional Anthropic client (for Claude codegen)

try:
from anthropic import Anthropic
except ImportError:
Anthropic = None  # type: ignore

# â€“â€“â€“â€“â€“ LOGGING â€“â€“â€“â€“â€“

logging.basicConfig(
level=os.getenv(â€œLOG_LEVELâ€, â€œINFOâ€),
format=â€™%(asctime)s - %(name)s - %(levelname)s - %(message)sâ€™
)
logger = logging.getLogger(**name**)

# â€“â€“â€“â€“â€“ CONSTANTS â€“â€“â€“â€“â€“

class Command(str, Enum):
CREATE = â€œCREATEâ€
SWITCH = â€œSWITCHâ€
FILES = â€œFILESâ€
MODEL = â€œMODELâ€
LLM = â€œLLMâ€
RUN = â€œRUNâ€
RESET = â€œRESETâ€
GENERATE = â€œGENERATEâ€
DOWNLOAD = â€œDOWNLOADâ€

class OutputPreference(str, Enum):
FILES_JSON = â€œFILES_JSONâ€
UNIFIED_DIFF = â€œUNIFIED_DIFFâ€
CODE_ONLY = â€œCODE_ONLYâ€

# â€“â€“â€“â€“â€“ CONFIGURATION â€“â€“â€“â€“â€“

@dataclass
class Config:
output_dir: Path
prompt_file_path: Path
adapter_model: str = â€œgpt-5â€
codegen_model_default: str = â€œgpt-5â€
adapter_targets: str = â€œPython 3.11; Ruff+Black; Pydantic v2; asyncio; type hints strictâ€
adapter_constraints: str = â€œNo secrets; reasonable perf; minimal deps; production qualityâ€
adapter_test_policy: str = â€œCOMPREHENSIVE_TESTSâ€
adapter_output_lang: str = â€œENâ€
adapter_output_pref: OutputPreference = OutputPreference.FILES_JSON
request_timeout: int = 300
max_file_size: int = 10_000_000  # 10MB
max_archive_size: int = 50_000_000  # 50MB

```
def __post_init__(self):
    self.output_dir.mkdir(parents=True, exist_ok=True)
    if not self.prompt_file_path.exists():
        raise FileNotFoundError(f"Prompt file not found: {self.prompt_file_path}")
```

config = Config(
output_dir=Path(os.getenv(â€œOUTPUT_DIRâ€, â€œ./outâ€)).resolve(),
prompt_file_path=Path(os.getenv(â€œPROMPT_FILE_PATHâ€, â€œ./config/prompt_adapter.jsonâ€)),
adapter_model=os.getenv(â€œADAPTER_MODELâ€, â€œgpt-5â€),
codegen_model_default=os.getenv(â€œCODEGEN_MODELâ€, â€œgpt-5â€),
adapter_targets=os.getenv(â€œADAPTER_TARGETSâ€, â€œPython 3.11; Ruff+Black; Pydantic v2; asyncio; type hints strictâ€),
adapter_constraints=os.getenv(â€œADAPTER_CONSTRAINTSâ€, â€œNo secrets; reasonable perf; minimal deps; production qualityâ€),
adapter_test_policy=os.getenv(â€œADAPTER_TEST_POLICYâ€, â€œCOMPREHENSIVE_TESTSâ€),
adapter_output_lang=os.getenv(â€œADAPTER_OUTPUT_LANGâ€, â€œENâ€),
adapter_output_pref=OutputPreference(os.getenv(â€œADAPTER_OUTPUT_PREFâ€, â€œFILES_JSONâ€)),
request_timeout=int(os.getenv(â€œREQUEST_TIMEOUTâ€, â€œ300â€)),
)

# â€“â€“â€“â€“â€“ MODELS CONFIG â€“â€“â€“â€“â€“

VALID_MODELS = {â€œgpt-5â€}
VALID_CODEGEN_MODELS = {
â€œgpt-5â€,
â€œclaude-opus-4-1-20250805â€,
}

# â€“â€“â€“â€“â€“ CLIENTS â€“â€“â€“â€“â€“

openai_client = OpenAI(
api_key=os.getenv(â€œOPENAI_API_KEYâ€),
timeout=config.request_timeout
)

anthropic_client = None
if os.getenv(â€œANTHROPIC_API_KEYâ€) and Anthropic is not None:
anthropic_client = Anthropic(api_key=os.getenv(â€œANTHROPIC_API_KEYâ€))

# â€“â€“â€“â€“â€“ FILE UTILITIES â€“â€“â€“â€“â€“

EXT2LANG = {
â€œ.pyâ€: â€œpythonâ€, â€œ.jsâ€: â€œjavascriptâ€, â€œ.tsâ€: â€œtypescriptâ€, â€œ.htmlâ€: â€œhtmlâ€,
â€œ.cssâ€: â€œcssâ€, â€œ.jsonâ€: â€œjsonâ€, â€œ.ymlâ€: â€œyamlâ€, â€œ.yamlâ€: â€œyamlâ€,
â€œ.shâ€: â€œbashâ€, â€œ.sqlâ€: â€œsqlâ€, â€œ.txtâ€: â€œtextâ€, â€œ.rsâ€: â€œrustâ€,
â€œ.goâ€: â€œgoâ€, â€œ.javaâ€: â€œjavaâ€, â€œ.cppâ€: â€œcppâ€, â€œ.câ€: â€œcâ€,
â€œ.rbâ€: â€œrubyâ€, â€œ.phpâ€: â€œphpâ€, â€œ.swiftâ€: â€œswiftâ€, â€œ.ktâ€: â€œkotlinâ€,
â€œ.jsxâ€: â€œjavascriptâ€, â€œ.tsxâ€: â€œtypescriptâ€, â€œ.mdâ€: â€œmarkdownâ€,
}

def detect_language(filename: str) -> str:
â€œâ€â€œDetect programming language from file extension.â€â€â€
return EXT2LANG.get(Path(filename).suffix.lower(), â€œtextâ€)

def sanitize_filename(filename: str) -> str:
â€œâ€â€œSanitize filename for safe filesystem operations.â€â€â€
unsafe_chars = [â€™..â€™, â€˜/â€™, â€˜\â€™, â€˜:â€™, â€˜*â€™, â€˜?â€™, â€˜â€â€™, â€˜<â€™, â€˜>â€™, â€˜|â€™]
clean_name = filename
for ch in unsafe_chars:
clean_name = clean_name.replace(ch, â€˜_â€™)

```
# Handle whitespace
clean_name = re.sub(r'\s+', '_', clean_name.strip())

# Limit length
max_length = 255
if len(clean_name) > max_length:
    name, ext = os.path.splitext(clean_name)
    clean_name = name[:max_length - len(ext)] + ext

# Ensure non-empty
if not clean_name or clean_name in ['.', '..']:
    clean_name = 'unnamed_file'

return clean_name
```

def safe_path_join(base_dir: Path, relative_path: str) -> Optional[Path]:
â€œâ€â€œSafely join paths preventing directory traversal.â€â€â€
try:
clean_path = relative_path.strip().lstrip(â€™/\â€™)

```
    # Check for dangerous patterns
    if '..' in clean_path or clean_path.startswith('/'):
        logger.warning(f"Potentially unsafe path rejected: {relative_path}")
        return None
    
    full_path = base_dir / clean_path
    
    # Ensure path stays within base directory
    try:
        full_path.resolve().relative_to(base_dir.resolve())
    except ValueError:
        logger.warning(f"Path outside base directory rejected: {relative_path}")
        return None
    
    return full_path
except Exception as e:
    logger.error(f"Error processing path: {e}")
    return None
```

def chat_dir(chat_id: int) -> Path:
â€œâ€â€œGet or create chat-specific directory.â€â€â€
p = config.output_dir / str(chat_id)
p.mkdir(parents=True, exist_ok=True)
return p

def latest_path(chat_id: int, filename: str) -> Path:
â€œâ€â€œGet path for latest version of a file.â€â€â€
return chat_dir(chat_id) / fâ€latest-{filename}â€

def ensure_latest_placeholder(chat_id: int, filename: str, language: str) -> Path:
â€œâ€â€œCreate placeholder file if it doesnâ€™t exist.â€â€â€
lp = latest_path(chat_id, filename)
if lp.exists():
return lp

```
stubs = {
    'python': "# -*- coding: utf-8 -*-\n# Auto-generated file\n",
    'javascript': "// Auto-generated file\n",
    'typescript': "// Auto-generated file\n",
    'html': "<!DOCTYPE html>\n<html>\n<head>\n    <title>Generated</title>\n</head>\n<body>\n</body>\n</html>\n",
    'css': "/* Auto-generated file */\n",
    'json': "{}\n",
    'yaml': "# Auto-generated file\n",
    'bash': "#!/usr/bin/env bash\n# Auto-generated file\n",
    'sql': "-- Auto-generated file\n",
    'rust': "// Auto-generated file\n",
    'go': "// Auto-generated file\npackage main\n",
    'java': "// Auto-generated file\npublic class Main {}\n",
    'text': "",
}

try:
    lp.write_text(stubs.get(language, ""), encoding="utf-8")
except Exception as e:
    logger.error(f"Failed to create placeholder: {e}")
    lp.touch()

return lp
```

def list_files(chat_id: int) -> List[str]:
â€œâ€â€œList all files in chat directory.â€â€â€
base = chat_dir(chat_id)
try:
return sorted([p.name for p in base.iterdir() if p.is_file()])
except Exception as e:
logger.error(fâ€Failed to list files: {e}â€)
return []

def version_current_file(chat_id: int, filename: str, new_content: str) -> Path:
â€œâ€â€œVersion a file with timestamp and update latest.â€â€â€
lp = latest_path(chat_id, filename)

```
# Check if content changed
old = lp.read_text(encoding="utf-8") if lp.exists() else ""
if hashlib.sha256(old.encode()).hexdigest() == hashlib.sha256(new_content.encode()).hexdigest():
    return lp

# Create versioned copy
ts = time.strftime("%Y%m%d-%H%M%S")
ver = chat_dir(chat_id) / f"{ts}-{filename}"

try:
    ver.write_text(new_content, encoding="utf-8")
    lp.write_text(new_content, encoding="utf-8")
    logger.info(f"Created version: {ver.name}")
except Exception as e:
    logger.error(f"Failed to version file: {e}")
    raise

return lp
```

# â€“â€“â€“â€“â€“ CODE EXTRACTION â€“â€“â€“â€“â€“

CODE_BLOCK_RE = re.compile(râ€`([a-zA-Z0-9_+-]+)?\n(.*?)`â€, re.DOTALL)
DIFF_BLOCK_RE = re.compile(râ€`(diff|patch)\n(.*?)`â€, re.DOTALL | re.IGNORECASE)
UNIFIED_DIFF_HINT_RE = re.compile(râ€(?m)^(â€” |+++ |@@ )â€)
GIT_DIFF_HINT_RE = re.compile(râ€(?m)^diff â€“git â€œ)

def extract_code(text: str) -> str:
â€œâ€â€œExtract code from markdown code block or return as-is.â€â€â€
m = CODE_BLOCK_RE.search(text)
return m.group(2).strip() if m else text.strip()

def extract_diff_and_spec(text: str) -> Tuple[str, str]:
â€œâ€â€œExtract diff blocks and remaining specification.â€â€â€
diff_parts: List[str] = []

```
def grab_diff(m: re.Match) -> str:
    diff_parts.append(m.group(2).strip())
    return ""

text_without_diff = DIFF_BLOCK_RE.sub(grab_diff, text)
diff_text = "\n\n".join(diff_parts).strip()

# Check for diff patterns outside code blocks
if not diff_text and (GIT_DIFF_HINT_RE.search(text_without_diff) or 
                      UNIFIED_DIFF_HINT_RE.search(text_without_diff)):
    return "", text_without_diff.strip()

return text_without_diff.strip(), diff_text
```

def is_placeholder_or_empty(content: str) -> bool:
â€œâ€â€œCheck if content is empty or placeholder.â€â€â€
if not content.strip():
return True
if â€œAuto-generatedâ€ in content or â€œcreated viaâ€ in content:
return True
return len(content.strip()) < 20

# â€“â€“â€“â€“â€“ SCHEMAS â€“â€“â€“â€“â€“

ADAPTER_JSON_SCHEMA = {
â€œnameâ€: â€œfinal_prompt_bundleâ€,
â€œschemaâ€: {
â€œtypeâ€: â€œobjectâ€,
â€œpropertiesâ€: {
â€œsystemâ€: {â€œtypeâ€: â€œstringâ€},
â€œdeveloperâ€: {â€œtypeâ€: â€œstringâ€},
â€œuserâ€: {â€œtypeâ€: â€œstringâ€},
â€œconstraintsâ€: {â€œtypeâ€: â€œstringâ€},
â€œnon_goalsâ€: {â€œtypeâ€: â€œstringâ€},
â€œtestsâ€: {â€œtypeâ€: â€œarrayâ€, â€œitemsâ€: {â€œtypeâ€: â€œstringâ€}},
â€œoutput_contractâ€: {â€œtypeâ€: â€œstringâ€}
},
â€œrequiredâ€: [â€œsystemâ€, â€œdeveloperâ€, â€œuserâ€, â€œconstraintsâ€, â€œtestsâ€, â€œoutput_contractâ€],
â€œadditionalPropertiesâ€: False
},
â€œstrictâ€: True
}

FILES_JSON_SCHEMA = {
â€œnameâ€: â€œcode_filesâ€,
â€œschemaâ€: {
â€œtypeâ€: â€œobjectâ€,
â€œpropertiesâ€: {
â€œfilesâ€: {
â€œtypeâ€: â€œarrayâ€,
â€œitemsâ€: {
â€œtypeâ€: â€œobjectâ€,
â€œpropertiesâ€: {
â€œpathâ€: {â€œtypeâ€: â€œstringâ€},
â€œcontentâ€: {â€œtypeâ€: â€œstringâ€}
},
â€œrequiredâ€: [â€œpathâ€, â€œcontentâ€],
â€œadditionalPropertiesâ€: False
}
}
},
â€œrequiredâ€: [â€œfilesâ€],
â€œadditionalPropertiesâ€: False
},
â€œstrictâ€: True
}

# â€“â€“â€“â€“â€“ ANTHROPIC API CALLS â€“â€“â€“â€“â€“

@retry(
stop=stop_after_attempt(3),
wait=wait_exponential(multiplier=1, min=1, max=8),
retry=retry_if_exception_type(Exception),
reraise=True
)
def anthropic_call(model: str, messages: List[Dict[str, str]]) -> str:
â€œâ€â€œCall Anthropic API with retry logic.â€â€â€
if anthropic_client is None:
raise RuntimeError(â€œAnthropic client not initialized. Set ANTHROPIC_API_KEYâ€)

```
# Separate system and user messages
system_parts: List[str] = []
user_parts: List[str] = []

for m in messages:
    role, content = m.get("role", "user"), m.get("content", "")
    if role == "system":
        system_parts.append(content)
    elif role == "user":
        user_parts.append(content)

system_text = "\n\n".join(filter(str.strip, system_parts))
user_text = "\n\n".join(filter(str.strip, user_parts))

logger.info(f"Calling Anthropic API with model {model}")

resp = anthropic_client.messages.create(
    model=model,
    system=system_text if system_text else None,
    temperature=0.0,
    max_tokens=8192,
    messages=[{"role": "user", "content": user_text}],
)

# Extract text from response
chunks: List[str] = []
for block in getattr(resp, "content", []):
    if hasattr(block, "text"):
        chunks.append(block.text)
    elif isinstance(block, dict) and block.get("type") == "text":
        chunks.append(block.get("text", ""))

return "".join(chunks).strip()
```

# â€“â€“â€“â€“â€“ PROMPT MANAGEMENT â€“â€“â€“â€“â€“

class PromptAdapterFile(BaseModel):
template: str
version: Optional[str] = None
description: Optional[str] = None

_PROMPT_CACHE: Dict[str, Any] = {â€œpathâ€: None, â€œmtimeâ€: None, â€œtemplateâ€: None}

def load_prompt_template() -> str:
â€œâ€â€œLoad and cache prompt template from file.â€â€â€
path = config.prompt_file_path
mtime = path.stat().st_mtime

```
# Use cache if valid
if (_PROMPT_CACHE["path"] == str(path) and 
    _PROMPT_CACHE["mtime"] == mtime and 
    _PROMPT_CACHE["template"]):
    return _PROMPT_CACHE["template"]

# Load and validate
data = json.loads(path.read_text(encoding="utf-8"))
cfg = PromptAdapterFile(**data)
tmpl = cfg.template

# Validate required tags
required_tags = ["<<<RAW_TASK>>>", "<<<MODE>>>", "<<<OUTPUT_PREF>>>", "<<<OUTPUT_LANG>>>"]
for tag in required_tags:
    if tag not in tmpl:
        raise ValueError(f"Prompt template missing required tag: {tag}")

# Update cache
_PROMPT_CACHE.update({"path": str(path), "mtime": mtime, "template": tmpl})
return tmpl
```

def render_adapter_prompt(
raw_task: str,
context_block: str,
mode_tag: str,
output_pref: str
) -> str:
â€œâ€â€œRender prompt template with substitutions.â€â€â€
template = load_prompt_template()

```
replacements = {
    "<<<RAW_TASK>>>": raw_task,
    "<<<CONTEXT>>>": context_block or "(none)",
    "<<<MODE>>>": mode_tag,
    "<<<TARGETS>>>": config.adapter_targets,
    "<<<CONSTRAINTS>>>": config.adapter_constraints,
    "<<<TEST_POLICY>>>": config.adapter_test_policy,
    "<<<OUTPUT_PREF>>>": output_pref,
    "<<<OUTPUT_LANG>>>": config.adapter_output_lang,
}

for key, value in replacements.items():
    template = template.replace(key, value)

return template
```

def build_context_block(chat_id: int, filename: str) -> str:
â€œâ€â€œBuild context block from existing file.â€â€â€
lp = latest_path(chat_id, filename)
if not lp.exists():
return â€œâ€

```
lang = detect_language(filename)
code = lp.read_text(encoding="utf-8")

return f"""<<<CONTEXT:FILE {filename}>>>
```

```{lang}
{code}
```

<<<END>>>â€â€â€

# â€“â€“â€“â€“â€“ QUALITY VALIDATION â€“â€“â€“â€“â€“

def validate_prompt_bundle(bundle: Dict[str, Any]) -> None:
â€œâ€â€œValidate adapter response quality.â€â€â€
required_fields = [â€œsystemâ€, â€œdeveloperâ€, â€œuserâ€, â€œconstraintsâ€, â€œtestsâ€, â€œoutput_contractâ€]

```
for field in required_fields:
    if field not in bundle or not str(bundle[field]).strip():
        raise ValueError(f"Adapter JSON missing or empty: {field}")

# Validate tests
if not isinstance(bundle["tests"], list) or len(bundle["tests"]) < 3:
    raise ValueError("Adapter JSON must contain at least 3 tests")

# Check for placeholders
text_concat = " ".join([
    bundle.get("system", ""),
    bundle.get("developer", ""),
    bundle.get("user", ""),
    bundle.get("constraints", ""),
    bundle.get("non_goals", ""),
    " ".join(bundle.get("tests", []))
]).lower()

forbidden_terms = ["todo", "placeholder", "tbd", "xxx", "fixme"]
for term in forbidden_terms:
    if term in text_concat:
        raise ValueError(f"Adapter JSON contains forbidden term '{term}' - not production ready")
```

# â€“â€“â€“â€“â€“ ADAPTER LOGIC â€“â€“â€“â€“â€“

def call_adapter(prompt_text: str) -> Dict[str, Any]:
â€œâ€â€œCall adapter and return structured prompt.â€â€â€
try:
logger.info(â€œCalling adapter with correct OpenAI Responses API formatâ€)

```
    # Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞ«Ğ™ Ğ²Ñ‹Ğ·Ğ¾Ğ² OpenAI Responses API
    resp = openai_client.responses.create(
        model=config.adapter_model,
        instructions="Return ONLY JSON per the schema. No prose outside JSON.",
        input=prompt_text,
        response_format={
            "type": "json_schema",
            "json_schema": ADAPTER_JSON_SCHEMA,
            "strict": True
        },
        reasoning={"effort": "medium"},
        text={"verbosity": "medium"},
        temperature=0.1,
        max_output_tokens=2000
    )
    
    # Extract parsed response
    bundle = None
    if hasattr(resp, "output_parsed") and resp.output_parsed:
        bundle = resp.output_parsed
    else:
        # Fallback to text parsing
        output_text = getattr(resp, "output_text", "")
        if not output_text:
            raise ValueError("Empty adapter response")
        
        try:
            bundle = json.loads(output_text)
        except json.JSONDecodeError:
            # Try to extract JSON from markdown block
            import re
            match = re.search(r'```(?:json)?\n?(.*?)```', output_text, re.DOTALL)
            if match:
                bundle = json.loads(match.group(1).strip())
            else:
                raise ValueError("Cannot parse adapter response as JSON")
    
    if not bundle:
        raise ValueError("Failed to get structured response from adapter")
    
    # Validate quality
    validate_prompt_bundle(bundle)
    
    # Build final messages
    dev_content = bundle["developer"].strip()
    extra = []
    
    if bundle.get("constraints"):
        extra.append(f"Constraints:\n{bundle['constraints'].strip()}")
    if bundle.get("non_goals"):
        extra.append(f"Non-Goals:\n{bundle['non_goals'].strip()}")
    
    tests = bundle.get("tests", [])
    if tests:
        extra.append("Acceptance Tests:\n- " + "\n- ".join(tests))
    
    if bundle.get("output_contract"):
        extra.append(f"Output Contract:\n{bundle['output_contract'].strip()}")
    
    if extra:
        dev_content += "\n\n" + "\n\n".join(extra)
    
    messages = [
        {"role": "system", "content": bundle["system"]},
        {"role": "system", "content": dev_content},
        {"role": "user", "content": bundle["user"]},
    ]
    
    return {
        "messages": messages,
        "response_contract": {"mode": bundle.get("output_contract", "FILES_JSON")},
        "constraints": bundle.get("constraints", ""),
        "non_goals": bundle.get("non_goals", ""),
        "tests": tests
    }
    
except Exception as e:
    logger.error(f"Adapter call failed: {e}", exc_info=True)
    # Fallback to simple prompt
    return {
        "messages": [
            {"role": "system", "content": "Generate production-quality code based on user request"},
            {"role": "user", "content": prompt_text}
        ],
        "response_contract": {"mode": config.adapter_output_pref.value}
    }
```

# â€“â€“â€“â€“â€“ CODEGEN LOGIC â€“â€“â€“â€“â€“

def get_provider_from_model(model: str) -> str:
â€œâ€â€œDetermine provider from model name.â€â€â€
return â€œanthropicâ€ if model.startswith(â€œclaudeâ€) else â€œopenaiâ€

def call_codegen(
messages: List[Dict[str, str]],
mode: Optional[str] = None,
model: str = â€œgpt-5â€
) -> str:
â€œâ€â€œCall code generation with specified model.â€â€â€
try:
provider = get_provider_from_model(model)

```
    if provider == "openai":
        # Convert messages to correct format for OpenAI Responses API
        system_parts = []
        user_parts = []
        
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if role == "system":
                system_parts.append(content)
            else:
                user_parts.append(content)
        
        instructions = "\n\n".join(system_parts) if system_parts else (
            "Generate clean, production-ready code based on user requirements."
        )
        input_text = "\n\n".join(user_parts)
        
        # Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞ«Ğ™ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸
        if mode and mode.upper() == "FILES_JSON":
            resp = openai_client.responses.create(
                model=model,
                instructions="Return ONLY JSON per the schema. Production-grade code; include tests.",
                input=input_text,
                response_format={
                    "type": "json_schema",
                    "json_schema": FILES_JSON_SCHEMA,
                    "strict": True
                },
                reasoning={"effort": "minimal"},
                text={"verbosity": "low"},
                temperature=0.1,
                max_output_tokens=3500
            )
            
            # Return structured JSON response
            if hasattr(resp, 'output_parsed') and resp.output_parsed:
                return json.dumps(resp.output_parsed, ensure_ascii=False, indent=2)
            else:
                output_text = getattr(resp, "output_text", "")
                if not output_text:
                    raise ValueError("Empty codegen output")
                return output_text
        else:
            # For non-JSON modes
            resp = openai_client.responses.create(
                model=model,
                instructions=instructions,
                input=input_text,
                reasoning={"effort": "minimal"},
                text={"verbosity": "low"},
                temperature=0.1,
                max_output_tokens=3500
            )
            
            output_text = getattr(resp, "output_text", "")
            if not output_text:
                raise ValueError("Empty codegen output")
            return output_text
    
    else:  # Anthropic
        txt = anthropic_call(model, messages)
        if not txt:
            raise ValueError("Empty codegen output from Anthropic")
        return txt
        
except Exception as e:
    logger.error(f"Codegen call failed: {e}", exc_info=True)
    return "# Error generating code"
```

# â€“â€“â€“â€“â€“ FILE APPLICATION â€“â€“â€“â€“â€“

def apply_files_json(
chat_id: int,
active_filename: str,
files_obj: List[Dict[str, str]]
) -> Path:
â€œâ€â€œApply FILES_JSON response to filesystem.â€â€â€
active_written = None
base_dir = chat_dir(chat_id)

```
for item in files_obj:
    raw_path = item.get("path", "").strip()
    content = item.get("content", "")
    
    if not raw_path:
        continue
    
    safe_output_path = safe_path_join(base_dir, raw_path)
    if safe_output_path is None:
        logger.warning(f"Skipping unsafe path: {raw_path}")
        continue
    
    try:
        # Check file size limit
        if len(content.encode('utf-8')) > config.max_file_size:
            logger.warning(f"File too large, skipping: {raw_path}")
            continue
        
        safe_output_path.parent.mkdir(parents=True, exist_ok=True)
        safe_output_path.write_text(content, encoding="utf-8")
        logger.info(f"Written file: {safe_output_path}")
        
        if Path(raw_path).name == active_filename:
            active_written = version_current_file(chat_id, active_filename, content)
            
    except Exception as e:
        logger.error(f"Failed to write file {raw_path}: {e}")
        continue

# Ensure active file is written
if active_written is None and files_obj:
    first = files_obj[0]
    content = first.get("content", "")
    active_written = version_current_file(chat_id, active_filename, content)

return active_written or latest_path(chat_id, active_filename)
```

def infer_output_preference(raw_text: str, has_context: bool) -> str:
â€œâ€â€œInfer output preference from text patterns.â€â€â€
if has_context and (DIFF_BLOCK_RE.search(raw_text) or
UNIFIED_DIFF_HINT_RE.search(raw_text) or
GIT_DIFF_HINT_RE.search(raw_text)):
return OutputPreference.UNIFIED_DIFF.value
return config.adapter_output_pref.value

# â€“â€“â€“â€“â€“ AUDIT LOGGING â€“â€“â€“â€“â€“

def audit_connect() -> sqlite3.Connection:
â€œâ€â€œConnect to audit database.â€â€â€
audit_db = config.output_dir / â€œaudit.dbâ€
conn = sqlite3.connect(audit_db)
conn.execute(â€â€â€
CREATE TABLE IF NOT EXISTS events (
id INTEGER PRIMARY KEY AUTOINCREMENT,
ts TEXT NOT NULL,
chat_id INTEGER NOT NULL,
event_type TEXT NOT NULL,
active_file TEXT,
model TEXT,
prompt TEXT,
output_path TEXT,
output_sha256 TEXT,
output_bytes INTEGER,
meta TEXT
)
â€œâ€â€)
conn.execute(â€œCREATE INDEX IF NOT EXISTS idx_chat_id ON events(chat_id)â€)
conn.execute(â€œCREATE INDEX IF NOT EXISTS idx_ts ON events(ts)â€)
return conn

def audit_event(
chat_id: int,
event_type: str,
active_file: Optional[str] = None,
model: Optional[str] = None,
prompt: Optional[str] = None,
output_path: Optional[Path] = None,
meta: Optional[Dict] = None
) -> None:
â€œâ€â€œLog audit event to database.â€â€â€
conn = audit_connect()
try:
ts = time.strftime(â€%Y-%m-%d %H:%M:%Sâ€)

```
    # Calculate file hash if exists
    sha, size = None, None
    if output_path and output_path.exists():
        size = output_path.stat().st_size
        sha = hashlib.sha256(output_path.read_bytes()).hexdigest()
    
    # Truncate prompt if too long
    if prompt and len(prompt) > 4000:
        prompt = prompt[:4000]
    
    conn.execute(
        """INSERT INTO events 
           (ts, chat_id, event_type, active_file, model, prompt, 
            output_path, output_sha256, output_bytes, meta)
           VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)""",
        (ts, chat_id, event_type, active_file, model, prompt,
         str(output_path) if output_path else None, sha, size,
         json.dumps(meta or {}))
    )
    conn.commit()
except Exception as e:
    logger.error(f"Failed to log audit event: {e}")
finally:
    conn.close()
```

# â€“â€“â€“â€“â€“ GRAPH STATE â€“â€“â€“â€“â€“

class GraphState(TypedDict, total=False):
chat_id: int
input_text: str
command: str
arg: Optional[str]
active_file: Optional[str]
model: str
codegen_model: str
pending_messages: Optional[List[Dict[str, str]]]
pending_mode: Optional[str]
pending_prompt_sha: Optional[str]
pending_context: Optional[str]
reply_text: str
file_to_send: Optional[str]
status_msgs: List[str]

# â€“â€“â€“â€“â€“ NODE DECORATOR â€“â€“â€“â€“â€“

def safe_node(func):
â€œâ€â€œDecorator for safe node execution with error handling.â€â€â€
def wrapper(state: GraphState) -> GraphState:
try:
return func(state)
except Exception as e:
logger.error(fâ€Error in {func.**name**}: {e}â€, exc_info=True)

```
        error_msg = f"âŒ Error in {func.__name__}: "
        if "api key" in str(e).lower():
            error_msg += "API key issue"
        elif "rate" in str(e).lower():
            error_msg += "Rate limit exceeded"
        elif "timeout" in str(e).lower():
            error_msg += "Request timeout"
        elif "json" in str(e).lower():
            error_msg += "JSON parsing error"
        else:
            error_msg += str(e)[:200]
        
        state["reply_text"] = error_msg
        return state

wrapper.__name__ = func.__name__
return wrapper
```

def push_status(state: GraphState, msg: str) -> None:
â€œâ€â€œAdd status message to state.â€â€â€
try:
if â€œstatus_msgsâ€ not in state:
state[â€œstatus_msgsâ€] = []

```
    if len(msg) > 500:
        msg = msg[:497] + "..."
    
    state["status_msgs"].append(msg)
except Exception as e:
    logger.error(f"Failed to push status: {e}")
```

# â€“â€“â€“â€“â€“ GRAPH NODES â€“â€“â€“â€“â€“

def parse_message(state: GraphState) -> GraphState:
â€œâ€â€œParse incoming message to determine command.â€â€â€
text = state[â€œinput_textâ€].strip()
state[â€œcommandâ€] = Command.GENERATE.value
state[â€œargâ€] = None

```
if text.startswith("/"):
    parts = text.split(maxsplit=1)
    cmd = parts[0].lower()
    arg = parts[1].strip() if len(parts) > 1 else None
    
    command_mapping = {
        "/create": Command.CREATE,
        "/switch": Command.SWITCH,
        "/files": Command.FILES,
        "/model": Command.MODEL,
        "/llm": Command.LLM,
        "/run": Command.RUN,
        "/reset": Command.RESET,
        "/download": Command.DOWNLOAD,
    }
    
    state["command"] = command_mapping.get(cmd, Command.GENERATE).value
    state["arg"] = arg

return state
```

@safe_node
def node_create(state: GraphState) -> GraphState:
â€œâ€â€œCreate or activate a file.â€â€â€
chat_id = state[â€œchat_idâ€]
raw_filename = (state.get(â€œargâ€) or â€œmain.pyâ€).strip()
filename = sanitize_filename(raw_filename)
language = detect_language(filename)

```
ensure_latest_placeholder(chat_id, filename, language)
state["active_file"] = filename
state.setdefault("codegen_model", config.codegen_model_default)

state["reply_text"] = f"âœ… File created/activated: {filename}\nğŸ“¤ Language: {language}"
push_status(state, f"âœ… Created/activated file {filename} (language: {language})")

if filename != raw_filename:
    state["reply_text"] += f"\nâš ï¸ Filename was sanitized for safety"

audit_event(chat_id, "CREATE", active_file=filename, model=state.get("model"))
return state
```

@safe_node
def node_switch(state: GraphState) -> GraphState:
â€œâ€â€œSwitch to an existing file.â€â€â€
chat_id = state[â€œchat_idâ€]
filename = (state.get(â€œargâ€) or â€œâ€).strip()

```
if not filename:
    state["reply_text"] = "Please specify filename: /switch app.py"
    return state

filename = sanitize_filename(filename)
if not latest_path(chat_id, filename).exists():
    state["reply_text"] = f"File {filename} doesn't exist. Use /create {filename} first."
    return state

state["active_file"] = filename
state["reply_text"] = f"ğŸ”€ Switched to {filename}"
push_status(state, f"ğŸ”€ Switched to file {filename}")

audit_event(chat_id, "SWITCH", active_file=filename, model=state.get("model"))
return state
```

@safe_node
def node_files(state: GraphState) -> GraphState:
â€œâ€â€œList all files in chat directory.â€â€â€
files = list_files(state[â€œchat_idâ€])

```
if not files:
    state["reply_text"] = "No files yet. Start with /create app.py"
else:
    state["reply_text"] = "ğŸ—‚ Files:\n" + "\n".join(f"- {f}" for f in files)

audit_event(state["chat_id"], "FILES", active_file=state.get("active_file"))
return state
```

@safe_node
def node_model(state: GraphState) -> GraphState:
â€œâ€â€œShow current model configuration.â€â€â€
cg_model = state.get(â€œcodegen_modelâ€) or config.codegen_model_default

```
state["reply_text"] = (
    f"ğŸ§  Adapter: GPT-5 (OpenAI Responses API)\n"
    f"   reasoning.effort=medium, text.verbosity=medium\n"
    f"ğŸ§© Codegen (default): {cg_model}\n"
    f"ğŸ”§ To select codegen model: /llm <{'|'.join(sorted(VALID_CODEGEN_MODELS))}> or /run"
)

audit_event(state["chat_id"], "MODEL", active_file=state.get("active_file"), model="gpt-5")
return state
```

@safe_node
def node_llm(state: GraphState) -> GraphState:
â€œâ€â€œSelect LLM for code generation and optionally run pending prompt.â€â€â€
chat_id = state[â€œchat_idâ€]
arg = (state.get(â€œargâ€) or â€œâ€).strip()
pending = state.get(â€œpending_messagesâ€)

```
logger.info(f"node_llm called for chat {chat_id} with arg: '{arg}', pending: {bool(pending)}")

if not arg:
    current = state.get("codegen_model") or config.codegen_model_default
    msg = (
        "Select a model for CODE GENERATION:\n"
        "Available models:\n- " + "\n- ".join(sorted(VALID_CODEGEN_MODELS)) +
        f"\n\nCurrent default: {current}"
    )
    if pending:
        msg += "\n\nğŸ’¡ You have a prepared prompt. After selecting, generation will start immediately."
    state["reply_text"] = msg
    return state

model = arg
if model not in VALID_CODEGEN_MODELS:
    state["reply_text"] = (
        f"Model not supported for codegen: {model}\n"
        f"Available: {', '.join(sorted(VALID_CODEGEN_MODELS))}"
    )
    return state

if model.startswith("claude") and anthropic_client is None:
    state["reply_text"] = (
        "Claude selected but Anthropic not configured. "
        "Please install 'anthropic' package and set ANTHROPIC_API_KEY."
    )
    return state

# Set as default
state["codegen_model"] = model
logger.info(f"Codegen model set to: {model}")

# If we have pending adapter, run immediately
if pending:
    try:
        mode = state.get("pending_mode") or config.adapter_output_pref.value
        messages = pending
        
        push_status(state, f"â–¶ï¸ Running codegen with selected model: {model}")
        logger.info(f"Starting codegen with model {model} for chat {chat_id}")
        
        codegen_text = call_codegen(messages, mode=mode, model=model)
        if not codegen_text or codegen_text == "# Error generating code":
            raise ValueError("Failed to generate code")
        
        active = state.get("active_file") or "main.py"
        updated_path = None
        
        # Apply based on mode
        if mode.upper() == "FILES_JSON":
            try:
                if codegen_text.strip().startswith('{'):
                    obj = json.loads(codegen_text)
                else:
                    obj = json.loads(extract_code(codegen_text))
            except json.JSONDecodeError as e:
                logger.error(f"JSON parse error: {e}")
                logger.error(f"Codegen text: {codegen_text[:500]}...")
                # Fallback: save as code
                code = extract_code(codegen_text)
                updated_path = version_current_file(chat_id, active, code)
                push_status(state, "âš ï¸ JSON parse failed, saved as code")
            else:
                files = obj.get("files", [])
                if not files:
                    raise ValueError("No files in response")
                
                push_status(state, f"ğŸ§© Applying FILES_JSON: {len(files)} file(s)")
                updated_path = apply_files_json(chat_id, active, files)
            
        elif mode.upper() == "UNIFIED_DIFF":
            push_status(state, "ğŸ§© Applying UNIFIED_DIFF (fallback: full replacement)")
            code = extract_code(codegen_text)
            updated_path = version_current_file(chat_id, active, code)
            
        else:
            push_status(state, "ğŸ§© Applying direct code output")
            code = extract_code(codegen_text)
            updated_path = version_current_file(chat_id, active, code)
        
        # Build response
        rel = latest_path(chat_id, active).relative_to(config.output_dir)
        status_lines = state.get("status_msgs", [])
        status_block = ""
        if status_lines:
            status_block = "ğŸ§­ Execution status:\n"
            status_block += "\n".join(f"{i+1}. {line}" for i, line in enumerate(status_lines))
            status_block += "\n\n"
        
        state["reply_text"] = (
            f"{status_block}"
            f"âœ… Updated {active} via PROMPT-ADAPTER v3\n"
            f"ğŸ§  Adapter: GPT-5 (OpenAI Responses API)\n"
            f"ğŸ§© Codegen LLM: {model}\n"
            f"ğŸ“„ Contract: {mode}\n"
            f"ğŸ’¾ Saved: {rel}\n\n"
            f"Commands: /files, /switch <file>, /download"
        )
        
        audit_event(
            chat_id, "GENERATE",
            active_file=active,
            model=model,
            prompt=json.dumps({
                "pending_mode": mode,
                "pending_sha": state.get("pending_prompt_sha"),
            }, ensure_ascii=False)[:4000],
            output_path=updated_path,
            meta={"adapter_ready": True, "success": True}
        )
        
        logger.info(f"Generation completed successfully for chat {chat_id}")
        
    except Exception as e:
        logger.exception(f"Error during codegen execution for chat {chat_id}")
        
        error_msg = str(e)
        if "json" in error_msg.lower():
            error_detail = "JSON parsing error - model returned invalid format"
        elif "api" in error_msg.lower():
            error_detail = "API connection issue"
        elif "rate" in error_msg.lower():
            error_detail = "Rate limit exceeded"
        elif "timeout" in error_msg.lower():
            error_detail = "Request timeout"
        else:
            error_detail = "Code generation failed"
        
        state["reply_text"] = (
            f"âŒ {error_detail}\n"
            f"Model: {model}\n"
            f"Details: {error_msg[:200]}\n\n"
            f"Try:\n"
            f"â€¢ `/run` - retry with same model\n"
            f"â€¢ `/llm gpt-5` - switch model\n"
            f"â€¢ `/reset` - start over"
        )
        
        audit_event(
            chat_id, "GENERATE_ERROR",
            active_file=state.get("active_file"),
            model=model,
            meta={"error": error_msg[:500]}
        )
        
    finally:
        # Clear pending state
        state.pop("pending_messages", None)
        state.pop("pending_mode", None)
        state.pop("pending_prompt_sha", None)
        state.pop("pending_context", None)
else:
    # No pending - just set default
    state["reply_text"] = f"ğŸ”§ Codegen model set to: {model} (will be used for next generation)"
    audit_event(chat_id, "LLM_SET", active_file=state.get("active_file"), model=model)

return state
```

@safe_node
def node_run(state: GraphState) -> GraphState:
â€œâ€â€œRun pending prompt with current model.â€â€â€
if not state.get(â€œpending_messagesâ€):
state[â€œreply_textâ€] = â€œNo prepared prompt. Send a task first for the adapter.â€
return state

```
model = state.get("codegen_model") or config.codegen_model_default
state["arg"] = model
return node_llm(state)
```

@safe_node
def node_reset(state: GraphState) -> GraphState:
â€œâ€â€œReset chat state.â€â€â€
state[â€œactive_fileâ€] = None
state[â€œmodelâ€] = config.adapter_model
state[â€œcodegen_modelâ€] = config.codegen_model_default

```
# Clear pending
state.pop("pending_messages", None)
state.pop("pending_mode", None)
state.pop("pending_prompt_sha", None)
state.pop("pending_context", None)

state["reply_text"] = "â™»ï¸ State reset. Start with /create <filename>"
audit_event(state["chat_id"], "RESET")
return state
```

@safe_node
def node_generate(state: GraphState) -> GraphState:
â€œâ€â€œGenerate code via adapter + codegen pipeline.â€â€â€
chat_id = state[â€œchat_idâ€]
active = state.get(â€œactive_fileâ€)

```
# Auto-create main.py if needed
if not active:
    active = "main.py"
    ensure_latest_placeholder(chat_id, active, detect_language(active))
    state["active_file"] = active
    logger.info(f"Auto-created file: {active}")

# Force adapter model
state["model"] = config.adapter_model
raw_user_text = state["input_text"]

# Check context
lp = latest_path(chat_id, active)
existed_before = lp.exists()
ensure_latest_placeholder(chat_id, active, detect_language(active))
current_text = lp.read_text(encoding="utf-8") if lp.exists() else ""
has_context = existed_before and not is_placeholder_or_empty(current_text)

context_block = build_context_block(chat_id, active) if has_context else ""
mode_tag = "DIFF_PATCH" if has_context else "NEW_FILE"
output_pref = infer_output_preference(raw_user_text, has_context)

push_status(state, f"ğŸ“© User request ({len(raw_user_text)} chars)")
push_status(state, f"ğŸ§  Adapter: GPT-5 (mode={mode_tag})")

try:
    # Call adapter
    adapter_prompt = render_adapter_prompt(raw_user_text, context_block, mode_tag, output_pref)
    push_status(state, f"âœ… Loaded external prompt: {config.prompt_file_path.resolve()}")
    
    sha = hashlib.sha256(adapter_prompt.encode('utf-8')).hexdigest()
    push_status(state, f"ğŸ“¤ Sending adapter prompt (hash: {sha[:10]}...)")
    
    adapter_result = call_adapter(adapter_prompt)
    messages = adapter_result.get("messages", [])
    if not messages:
        raise ValueError("Adapter returned empty messages")
    
    mode = adapter_result.get("response_contract", {}).get("mode", output_pref)
    
    # Store pending and ask user to choose model
    state["pending_messages"] = messages
    state["pending_mode"] = mode
    state["pending_prompt_sha"] = sha
    state["pending_context"] = "present" if has_context else "absent"
    
    audit_event(
        chat_id, "ADAPTER_READY",
        active_file=active,
        model=config.adapter_model,
        prompt=json.dumps({"mode": mode, "sha": sha[:16]}, ensure_ascii=False)[:4000],
        meta={"has_context": has_context}
    )
    
    # Build response
    options = " | ".join(sorted(VALID_CODEGEN_MODELS))
    status_lines = state.get("status_msgs", [])
    status_block = ""
    if status_lines:
        status_block = "ğŸ§­ Status:\n"
        status_block += "\n".join(f"{i+1}. {line}" for i, line in enumerate(status_lines))
        status_block += "\n\n"
    
    default_model = state.get("codegen_model") or config.codegen_model_default
    state["reply_text"] = (
        f"{status_block}"
        "âœ… Structured prompt ready.\n"
        "Choose LLM for code generation (user decision - no auto-run):\n"
        f"â†’ /llm <{options}>  (recommended)\n"
        f"â†’ /run  (use current: {default_model})\n\n"
        "After selection, generation and file updates will begin."
    )
    
except Exception as e:
    logger.error(f"Adapter stage failed: {e}", exc_info=True)
    audit_event(
        chat_id, "ADAPTER_ERROR",
        active_file=active,
        model=config.adapter_model,
        meta={"error": str(e)[:500]}
    )
    raise

return state
```

@safe_node
def node_download(state: GraphState) -> GraphState:
â€œâ€â€œCreate downloadable archive.â€â€â€
chat_id = state[â€œchat_idâ€]
arg = state.get(â€œargâ€)

```
try:
    archive_path = make_archive(chat_id, arg)
    state["file_to_send"] = str(archive_path)
    selection = arg or "all"
    state["reply_text"] = f"ğŸ“¦ Prepared archive {archive_path.name} ({selection})"
    push_status(state, f"ğŸ“¦ Created archive {archive_path.name} (filter: {selection})")
    
    audit_event(
        chat_id, "DOWNLOAD",
        active_file=state.get("active_file"),
        model=state.get("codegen_model") or config.codegen_model_default,
        output_path=archive_path
    )
except Exception as e:
    logger.error(f"Failed to create archive: {e}")
    state["reply_text"] = f"âŒ Failed to create archive: {str(e)[:200]}"

return state
```

# â€“â€“â€“â€“â€“ ARCHIVE CREATION â€“â€“â€“â€“â€“

def iter_selected_files(base: Path, arg: Optional[str]) -> Iterable[Path]:
â€œâ€â€œIterate files based on selection criteria.â€â€â€
try:
files = [p for p in base.iterdir() if p.is_file()]
if not arg:
return sorted(files)

```
    arg = arg.strip().lower()
    if arg == "latest":
        return sorted([p for p in files if p.name.startswith("latest-")])
    elif arg == "versions":
        return sorted([p for p in files if not p.name.startswith("latest-")])
    else:
        # Specific file pattern
        return sorted([
            p for p in files 
            if p.name == f"latest-{arg}" or p.name.endswith(f"-{arg}")
        ])
except Exception as e:
    logger.error(f"Failed to select files: {e}")
    return []
```

def make_archive(chat_id: int, arg: Optional[str]) -> Path:
â€œâ€â€œCreate ZIP archive of selected files.â€â€â€
base = chat_dir(chat_id)
ts = time.strftime(â€%Y%m%d-%H%M%Sâ€)
out = base / fâ€export-{ts}.zipâ€

```
to_pack = list(iter_selected_files(base, arg))
if not to_pack:
    raise ValueError("No files to archive")

# Check total size
total_size = sum(p.stat().st_size for p in to_pack)
if total_size > config.max_archive_size:
    raise ValueError(f"Archive too large: {total_size} bytes")

with zipfile.ZipFile(out, "w", compression=zipfile.ZIP_DEFLATED) as z:
    for p in to_pack:
        z.write(p, arcname=p.name)

logger.info(f"Created ZIP archive: {out.name} with {len(to_pack)} files")
return out
```

# â€“â€“â€“â€“â€“ GRAPH BUILDER â€“â€“â€“â€“â€“

def router(state: GraphState) -> str:
â€œâ€â€œRoute to appropriate node based on command.â€â€â€
return state[â€œcommandâ€]

def build_app() -> Any:
â€œâ€â€œBuild the LangGraph application.â€â€â€
sg = StateGraph(GraphState)

```
# Add nodes
sg.add_node("parse", parse_message)
sg.add_node("CREATE", node_create)
sg.add_node("SWITCH", node_switch)
sg.add_node("FILES", node_files)
sg.add_node("MODEL", node_model)
sg.add_node("LLM", node_llm)
sg.add_node("RUN", node_run)
sg.add_node("RESET", node_reset)
sg.add_node("GENERATE", node_generate)
sg.add_node("DOWNLOAD", node_download)

# Set entry point
sg.set_entry_point("parse")

# Add conditional routing
sg.add_conditional_edges(
    "parse",
    router,
    {
        Command.CREATE.value: "CREATE",
        Command.SWITCH.value: "SWITCH",
        Command.FILES.value: "FILES",
        Command.MODEL.value: "MODEL",
        Command.LLM.value: "LLM",
        Command.RUN.value: "RUN",
        Command.RESET.value: "RESET",
        Command.GENERATE.value: "GENERATE",
        Command.DOWNLOAD.value: "DOWNLOAD",
    }
)

# Add edges to END
for node in ["CREATE", "SWITCH", "FILES", "MODEL", "LLM", "RUN", "RESET", "GENERATE", "DOWNLOAD"]:
    sg.add_edge(node, END)

# Initialize checkpointer
checkpointer = MemorySaver()
logger.info("Using MemorySaver for state management")

# Compile graph
compiled_app = sg.compile(checkpointer=checkpointer)
logger.info("LangGraph application compiled successfully")

return compiled_app
```

# â€“â€“â€“â€“â€“ INITIALIZATION â€“â€“â€“â€“â€“

APP = build_app()

**all** = [â€˜APPâ€™, â€˜VALID_MODELSâ€™, â€˜VALID_CODEGEN_MODELSâ€™, â€˜configâ€™]

logger.info(
â€œGraph app initialized. Adapter: %s. Codegen models: %s. Output dir: %sâ€,
config.adapter_model,
sorted(VALID_CODEGEN_MODELS),
config.output_dir
)